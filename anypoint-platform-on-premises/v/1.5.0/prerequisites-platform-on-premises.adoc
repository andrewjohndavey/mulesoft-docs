= Prerequisites for Installing the Anypoint Platform On-premises Edition
:keywords:anypoint platform, on premises


This section contains hardware, software and networking requirements for installing the link:/anypoint-platform-on-premises/v/1.5.0/index[Anypoint Platform on-Premises Edition].

If you comply with the necessary specs, see the link:/anypoint-platform-on-premises/v/1.5.0/installing-anypoint-on-premises[installation document]



== Hardware requirements

For production usage, it’s recommend to have at least 3 servers with the following minimum requirements each:

[%header%autowidth.spread]
|===
| Requirement |Value
|RAM |32GB
|CPU |8 Cores
|HDD 1 (OS) |50GB
|HDD 2 (for System data) | 100GB
|HDD 3 (for Application data)| 250GB
|HDD 4 (for Docker) | 100GB
|HDD 5 (for etcd) | 20GB
|Networking (connectivity between hosts)  |1GbE
|===

When using virtual machines, make sure that they are using different physical servers, otherwise HA won’t work properly.

=== Disk performance

All disks should be relatively performant, even if they are virtual block devices, or network attached storage. You can measure disk throughput with a tool such as `hdparm`. On CentOS `hdparm` can be installed by running `sudo yum install -y hdparm`.

==== Throughput

All disks should have at least 300MB/sec of throughput. You can verify this with the following command:

----
sudo hdparm -d <device>
----

For example:

----
$ sudo hdparm -t /dev/sdd

/dev/sdd:
 Timing buffered disk reads: 4726 MB in  3.00 seconds = 1574.94 MB/sec
----

Another way of measuring disk throughput is with the tool `dd`. `dd` writes directly to the specified file, even if it is a device, so you'll want to be cautious and not use any of your bare devices. Instead, once a device is both formatted and mounted, you can write to a file on that device to measure throughput.

----
$ sudo mount /dev/sdb /var/lib/gravity # must be already formatted!
$ sudo dd if=/dev/zero of=/var/lib/gravity/testfile count=1000 bs=1M
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB, 1000 MiB) copied, 0.382535 s, 2.7 GB/s
----

`dd` will provide slightly less accurate information than a tool such as `hdparm`, but it is readily available on any machine, and provides the ability to easily verify disk performance generally.

==== IOPS

Depending on your hardware or virtualization provider, you may be able to configure disk IOPS (I/O Operations Per Second).

We recommend the following performance quotas:

[%header%autowidth.spread]
|===
| Disk |Recommended IOPS
|HDD 1 (OS) |500
|HDD 2 (for System data) | 1500
|HDD 3 (for Application data)| 1500
|HDD 4 (for Docker) | 3000
|HDD 5 (for etcd) | 3000
|===

Using a tool like `iops` (available here https://benjamin-schweizer.de/files/iops/iops-2011-02-11), you can verify available IOPS:

----
$ sudo ./iops --time 2 /dev/xvdb
/dev/xvdb, 107.37 GB, 32 threads:
 512   B blocks: 1893.0 IO/s, 946.5 KiB/s (  7.8 Mbit/s)
   1 KiB blocks: 1354.8 IO/s,   1.3 MiB/s ( 11.1 Mbit/s)
   2 KiB blocks: 1091.8 IO/s,   2.1 MiB/s ( 17.9 Mbit/s)
   4 KiB blocks:  807.1 IO/s,   3.2 MiB/s ( 26.4 Mbit/s)
   8 KiB blocks:  803.7 IO/s,   6.3 MiB/s ( 52.7 Mbit/s)
  16 KiB blocks:  787.4 IO/s,  12.3 MiB/s (103.2 Mbit/s)
  32 KiB blocks:  700.8 IO/s,  21.9 MiB/s (183.7 Mbit/s)
  64 KiB blocks:  590.0 IO/s,  36.9 MiB/s (309.3 Mbit/s)
 128 KiB blocks:  327.6 IO/s,  40.9 MiB/s (343.5 Mbit/s)
...
----

=== VMWare instructions

When setting up a VMWare virtual machine, make sure to specify the root device as 50GB (or bigger) as follows:

image:vmware_root_disk_setup_1.png["Disk setup"]

image:vmware_root_disk_setup_2.png["Disk setup"]

Once finished, you should see a virtual machine that looks similar to the following:

image:vmware_root_disk_setup_3.png["Initial VM settings"]

Next, you will need to add the 3 other devices required for installation (HDD2, HDD3 and HDD4). To do this, click the "Add..." button, and select "Hard Disk":

image:vmware_add_new_hd_1.png["Select Hard Disk"]

Select "Create a new virtual disk":

image:vmware_add_new_hd_2.png["Create a new virtual disk"]

Add set the size to 100GB for HDD2.

image:vmware_add_new_hd_3.png["Size this disk to 100GB"]

Once finished adding that disk, create another disk for HDD3, this time at least 250GB:

image:vmware_add_new_hd_4.png["Size this disk to 250GB"]

Next, create another disk for HDD4, this time at least 100GB:

image:vmware_add_new_hd_5.png["Size this disk to 100GB"]

Finally, create another disk for HDD5, this time 20GB:

image:vmware_add_new_hd_6.png["Size this disk to 20GB"]

When finished, your virtual machine should show 5 disks, similar to the following view:

image:vmware_done_disks.png["Finished virtual machine settings"]

Proceed with installation of Operating System and application as normal.

== Software Requirements

=== Distributions

The following distributions are supported:

* RHEL 7.2.x
* CentOS 7.2.x

=== SELinux

SELinux must be disabled, or set to permissive mode.

=== Packages to install

*Yum* is an open-source command-line package-management utility for Linux operating systems using the RPM Package Manager.

Through Yum, install the tool LVM. LVM (orLogical Volume Manager) is a tool that adds a layer of abstraction between your operating system and the disks/partitions it uses. You can install LVM through the following command:

----
`sudo yum install lvm2`
----

[NOTE]
You must use a user with root access to perform this installation.

=== Packages to uninstall

==== Docker

Docker should be uninstalled from the servers running the Anypoint Platform On-premises Edition. The Anypoint Platform installation includes its own packaging of Docker, officially supported by Kubernetes.

==== Local name service

Local caching DNS servers listening on port 53 should be removed, e.g. named, dnsmasq, bind or others.


==== Server settings

Make sure that server running installer and servers in the cluster are set to UTC timezone:

----
sudo unlink /etc/localtime
sudo ln -s /usr/share/zoneinfo/Etc/GMT /etc/localtime
----

== Networking Requirements

=== Static IPs

All servers in the cluster should have static private IPv4 assigned to them, these must persist between server restarts. If IPs don’t persist between reboots, the cluster will enter a failed state.

=== VXLAN

This version of Kubernetes sets up overlay VXLAN and uses UDP transport to encapsulate traffic.

There’s direct communication between components of the cluster via TCP. The table below shows the ports used for inter-host communication:

[%header%autowidth.spread]
|===
|Protocol |Port/Range |Purpose
|TCP | 6060 | Health check
|TCP |7469 |Cluster control plane
|UDP |8472 |Overlay VXLAN network
|TCP |6443 |Kubernetes API server
|TCP |8080 |Kubernetes API server
|TCP |10248-10255 |Kubernetes Kubelet
|TCP |2379, 2380, 4001, 7001 |etcd distributed database
|TCP | 5000 | Docker registry
|TCP |3008-3010, 3023-3025, 3080, 7575|cluster control plane
|TCP |30000-32767 |Internal services port range
|TCP | 7000, 7011, 7199, 9042, 9160 | Cassandra
|TCP | 18080, 18443 | Object store cluster
|TCP | 5431-5435 | Database cluster
|TCP |61008-61010 | Installer port ranges (only used during install)
|TCP |61022-61024 | Installer port ranges (only used during install)
|===

=== NAT Traffic

Kubernetes overlay network uses NAT in some cases. This requires that servers should be able to send and receive packages with a source and destination that is different from server’s internal IP.

=== SSL Certificate

In order to use the Anypoint Platform, you must provide SSL credentials. You can upload a certificate through the Anypoint Platform UI, see link:/access-management/on-premises-features#security[on-prem features]. This certificate must be trusted by every machine that’s connected to the platform.

[NOTE]
Keep in mind that you must register the same SSL certificate on every server with Mule Runtimes that are managed by this platform.

=== SMTP Server

Your network must include an SMTP server to manage e-mail alerts that are triggered by the platform. See link:/access-management/on-premises-features#smtp[on-prem features].

== Device Requirements

For the platform’s configuration you must assign two dedicated devices for use. One as a system state directory and the other as a target for Docker devicemapper configuration. These two directories must exist on every node of your cluster.

=== Anypoint system data device

The main purpose of the system state directory is storing system configuration and metadata - for example, database and packages among other things. As package sizes can be arbitrary large, it is important to estimate the minimum size requirements and allocate enough space as a dedicated device ahead of time.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity.mount
sudo systemctl start var-lib-gravity.mount
----

=== Anypoint application data device

The main purpose of applicaiton data directory is storing application configuration and data. The amount of space required should be at minimum 250GB, but might vary depending on your specific usecase. It is important to estimate the minimum size requirements and allocate enough space as a dedicated device ahead of time.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/data`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/data
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/data\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-data.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-data.mount
sudo systemctl start var-lib-data.mount
----

=== Docker device

This device is used by Docker’s Device Mapper storage driver.

Unless specified, Docker configuration defaults to the use of Device Mapper in loopback mode (using /dev/loopX devices) which is not recommended for production. To configure Docker to use a dedicated device for Device Mapper storage driver, an unformatted device (or a partition) (i.e. /dev/sdd) can be provided during installation. This directory will be automatically configured and set up for use.

Unformatted devices potentially usable for system directory / Device Mapper are automatically discovered by agents running on each node. Discovered devices are offered on a drop-down menu for configuration before the installation is started.

=== Etcd device

The main purpose of the etcd device is to provide dedicated storage for a distributed database used for cluster coordination. It does not require much space, 20GB should be enough.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity/planet/etcd`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity/planet/etcd
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity/planet/etcd\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity-planet-etcd.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity-planet-etcd.mount
sudo systemctl start var-lib-gravity-planet-etcd.mount
----


[TIP]
====
You can list unmounted devices with the following command:
---
lsblk --output=NAME,TYPE,SIZE,FSTYPE -P -I 8,9,202|grep 'FSTYPE=""'
---
====

The unmounted devices have an empty value in FSTYPE column. Devices with TYPE="part" are partitions on another device.
Note, that we only list specific device types:
|===
|Device type|Description
|8   |SCSI disk devices
|9   |Metadisk (RAID) devices
|202 |Xen virtual block devices (Amazon EC2)
|===

==== Manually resetting devices/partitions
Logical Volume Manager allows one to group multiple physical volumes into a single storage volume (Volume Group) and then divide these into Logical
Volumes. Physical Volumes are either a whole device or a partition.

In rare cases when a device is in use by another Logical Volume or you want to manually reset a device previously configured for Device Mapper, here is
a list of commands that will help:

Logical Volume Manager toolset consists of the following commands:
  * dmsetup - is a low-level logical volume management
  * pv/vg/lv-prefixed commands like pvdisplay and pvcreate/pvremove - for working with specific LVM object types (i.e. lv - for logical volumes and vg - for volume groups)

In order to reset a device, you will have to work backwards:
  * remove logical volume with `lvremove -f docker/thinpool` (use `lvdisplay` to find the volume to remove)
  * remove volume group with `vgremovei docker` (use `vgdisplay` to locate the volume group to remove)
  * remove physical volume and reset device with `pvremove /dev/<device name>` (use `pvdisplay` to find the physical volume to remove and the device name it is on)

==== See Also
 link:https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/[Docker and the DeviceMapper Storage Driver]
 link:https://linuxconfig.org/linux-lvm-logical-volume-manager[Linux lvm - Logical Volume Manager]
 link:https://www.kernel.org/doc/Documentation/devices.txt[Linux allocated devices]

[NOTE]
It is strongly recommended to have at least 100Gb sized device for the Device Mapper directory - with devices 50Gb and less the system performance will degrade dramatically or might not work at all.
